# -*- coding: utf-8 -*-
"""Multilingual_Named_Entity_Recognition_For_Indian_Languages_Formatted.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Nnx9KnJUBbm55BuHAO9oSzWr9BVX3KVt

<a href="https://colab.research.google.com/github/shrishtinigam/multilingual_named_entity_recognition/blob/main/Multilingual_Named_Entity_Recognition_For_Indian_Languages.ipynb" target="_parent"><img src="https://colab.research.google.com/assets/colab-badge.svg" alt="Open In Colab"/></a>
"""

!pip install datasets transformers[sentencepiece]

"""## Loading Datasets

We will be using a subset of the Cross-lingual TRansfer Evaluation of Multilingual Encoders
(XTREME) benchmark called Wikiann or PAN-X. This dataset consists of Wikipedia articles in many
languages.
"""

from datasets import load_dataset
load_dataset("xtreme", "PAN-X.de", data_dir="data")

"""To make a representative Indian corpus,
we sample the Hindi, Bengali, Marathi, Telegu, Tamil, and Malyalam corpora from PAN-X according to their spoken proportions.

The fraction of Indian languages (in terms of mother tongue) Hindi, Bengali, Marathi, Telegu, Tamil, and Malyalam spoken are 43.63, 8.03, 6.86, 6.7, 5.7, 2.88. The rest of the % are an assortment of other languages. These percentages can be written as 59.11, 10.88, 9.3, 9.08, 7.73, 3.9 out of 100.

"""

from collections import defaultdict
from datasets import DatasetDict

langs = ["hi", "bn", "mr", "te", "ta", "ml"]

fracs = [0.5911, 0.1088, 0.093, 0.0908, 0.0773,0.039]

"""To keep track of each language, we create a Python defaultdict that stores
the language code as the key anda PAN-X corpus of type DatasetDict as the value.
We use the Dataset.shuffle function to make sure we don’t accidentally bias our
dataset splits
"""

# return a DatasetDict if a key doesn't exist
panx_ch = defaultdict(DatasetDict)
for lang, frac in zip(langs, fracs):
 # load monolingual corpus
 ds = load_dataset("xtreme", f"PAN-X.{lang}", data_dir="data")
 # shuffle and downsample each split according to spoken proportion
 for split in ds.keys():
  panx_ch[lang][split] = (ds[split].shuffle(seed=0).select(range(int(frac * ds[split].num_rows))))

"""Number of examples we have per language:"""

import pandas as pd
pd.DataFrame({lang: [panx_ch[lang]["train"].num_rows] for lang in langs},
 index=["Number of training examples"])

"""Inspecting one of the examples in the Hindi corpus"""

panx_ch["hi"]["train"][0]

panx_ch["hi"]["train"].features

tags = panx_ch["hi"]["train"].features["ner_tags"].feature
tags

tags.str2int("B-PER")

tags.int2str(1)

"""We use the ClassLabel.int2str function to create a new column in our training set with class names for
each tag. <br> <br>
We use the Dataset.map function to return a dict with the key corresponding to the new column
name and the value as a list of class names:

"""

def create_tag_names(batch):
 return {"ner_tags_str": [tags.int2str(idx) for idx in batch["ner_tags"]]}
panx_hi = panx_ch["hi"].map(create_tag_names)

hi_example = panx_hi["train"][35]
df = pd.DataFrame([hi_example["tokens"], hi_example["ner_tags_str"]], ['Tokens', 'Tags'])
df

"""Calculating the frequencies of each
entity across each split:

"""

from itertools import chain
from collections import Counter
split2freqs = {}
for split in panx_hi.keys():
  tag_names = []
  for row in panx_hi[split]["ner_tags_str"]:
    tag_names.append([t.split("-")[1] for t in row if t.startswith("B")])
  split2freqs[split] = Counter(chain.from_iterable(tag_names))
pd.DataFrame.from_dict(split2freqs, orient="index")

"""Visualizing how the autotokenizer works"""

from transformers import AutoTokenizer
bert_model_name = "bert-base-cased"
xlmr_model_name = "xlm-roberta-base"
bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)
xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)

text = "Jack Sparrow loves New York!"
bert_tokens = bert_tokenizer(text).tokens()
xlmr_tokens = xlmr_tokenizer(text).tokens()

"".join(xlmr_tokens).replace("▁", " ") #  Unicode symbol U+2581

"""## Model Bodies and Heads
The main concept that makes Transformers so versatile is the split of the architecture into a body and head. <br> To switch from the pretraining task to the downstream task, we need to replace the
last layer of the model with one that is suitable for the task. <br> This last layer is called the **model head** and is the part
that is **task specific**. <br> The rest of the model is called the **body** and includes the token embeddings and Transformer
layers that are **task agnostic**.

## Building user-defined XLM-R class for token classification:
We initialize a new model using config_class. This applies the standard XLM-R settings.
<br><br>
The **super()** function calls the initialization function of RobertaPreTrainedModel.
<br><br>
To define our model architecture, we take the model body from RobertaModel and extend it with our own classification head. <br>
Our **classification head** consists of a **dropout** and a **standard feedforward layer**.
<br><br>
We call **init_weights** to initialize all the weights in the model. This function loads the pretrained weights for the model body and randomly initializes the weights of the token classification head.
"""

import torch.nn as nn
from transformers import XLMRobertaConfig
from transformers.models.roberta.modeling_roberta import (RobertaModel, RobertaPreTrainedModel)
class XLMRobertaForTokenClassification(RobertaPreTrainedModel):
  config_class = XLMRobertaConfig
  def __init__(self, config):
    super().__init__(config)
    self.num_labels = config.num_labels
    # load model body
    self.roberta = RobertaModel(config, add_pooling_layer=False)
    # setup token classification head
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, config.num_labels)
    # load and initialize weights
    self.init_weights()
  def forward(self, input_ids=None, attention_mask=None,
 token_type_ids=None, labels=None, **kwargs):
    pass

"""## Forward Pass  
data -> model body -> model head -> dropout layer ->  classification layer -> output

<br>
During the forward pass the data is first fed through the model body.  
<br>
The hidden state, which is part of the model body output, is then fed through the dropout and
classification layer. We calculate the loss.
<br><br>
Finally, we wrap all the outputs in a TokenClassifierOutput object.
"""

from transformers.modeling_outputs import TokenClassifierOutput
def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):
 # use model body to get encoder representations
 outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)
 # apply classifier to encoder representation
 sequence_output = self.dropout(outputs[0])
 logits = self.classifier(sequence_output)
 # calculate losses
 loss = None
 if labels is not None:
  loss_fct = nn.CrossEntropyLoss()
  loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))
 # return model output object
 return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)

# Updating our placeholder "pass" forward function with our user defined forward function
XLMRobertaForTokenClassification.forward = forward

"""Tags and the indices that represent them are -
(IOB2 Tagging)
"""

index2tag = {idx: tag for idx, tag in enumerate(tags.names)}
tag2index = {tag: idx for idx, tag in enumerate(tags.names)}

index2tag

tag2index

"""### Loading the XLM-R configuration for NER"""

from transformers import AutoConfig
xlmr_config = AutoConfig.from_pretrained(xlmr_model_name,num_labels=tags.num_classes, id2label=index2tag, label2id=tag2index)

"""Loading weights with the "from_pretrained" function. (Inherited from the RobertaPretrainedModel, not user implemented.)"""

import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
xlmr_model = (XLMRobertaForTokenClassification.from_pretrained(xlmr_model_name, config=xlmr_config).to(device))

"""At this point, technically we can test the predictions. The weights for the model body are from the RobertaModel and the weights for the model head are random. Thus we can see what the pretrained model predicts. Unsuprisingly, it is not very accurate."""

input_ids = xlmr_tokenizer.encode(text, return_tensors="pt").to(device)

df = pd.DataFrame([xlmr_tokenizer.all_special_tokens,
 xlmr_tokenizer.all_special_ids],
 index=["Special Token", "Special Token ID"])
df

"""Passing the inputs to the model and extracting the predictions by taking the argmax to get the most
likely class per token
"""

outputs = xlmr_model(input_ids).logits
predictions = torch.argmax(outputs, dim=-1)
print(f"Number of tokens in sequence: {len(xlmr_tokens)}")
print(f"Shape of outputs: {outputs.shape}")

"""Printing those tokens - it is not very accurate."""

preds = [tags.names[p] for p in predictions[0].cpu().numpy()]
df = pd.DataFrame([xlmr_tokens, preds], index=["Tokens", "Tags"])
df

"""Wrapping the above prediction steps into a helper function for later use:"""

def tag_text(text, tags, model, tokenizer):
 # get tokens with special characters
 tokens = tokenizer.tokenize(tokenizer.decode(tokenizer.encode(text)))
 # encode the sequence into IDs
 inputs = tokenizer.encode(text, return_tensors="pt").to(device)
 # get predictions as distribution over 7 possible classes
 outputs = model(inputs)[0]
 # take argmax to get most likely class per token
 predictions = torch.argmax(outputs, dim=2)
 # convert to DataFrame
 preds = [tags.names[p] for p in predictions[0].cpu().numpy()]
 df = pd.DataFrame([tokens, preds], index=["Tokens", "Tags"])
 df

"""## Tokenizing and Encoding the Texts

XLM-R's tokenizer returns the input ID's for the model's inputs. We augment this with attention mask and label ID's to encode the info about **which token is associated with which NER tag.**
"""

words, labels = hi_example["tokens"], hi_example["ner_tags"]

tokenized_input = xlmr_tokenizer(hi_example["tokens"], is_split_into_words=True)
tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input["input_ids"])

word_ids = tokenized_input.word_ids()
word_ids

""" We set -100 as the label for these special tokens and the subwords we wish to mask during training."""

previous_word_idx = None
label_ids = []
for word_idx in word_ids:
  if word_idx is None:
    label_ids.append(-100)
  elif word_idx != previous_word_idx:
    label_ids.append(labels[word_idx])
  else:
    label_ids.append(-100)
  previous_word_idx = word_idx

"""Scaling this out to the whole dataset by defining the following function -"""

def tokenize_and_align_labels(examples):
  tokenized_inputs = xlmr_tokenizer(examples["tokens"], truncation=True, is_split_into_words=True)

  labels = []

  for idx, label in enumerate(examples["ner_tags"]):
    word_ids = tokenized_inputs.word_ids(batch_index=idx)
    previous_word_idx = None
    label_ids = []
    for word_idx in word_ids:
      if word_idx is None or word_idx == previous_word_idx:
        label_ids.append(-100)
      else:
        label_ids.append(label[word_idx])
      previous_word_idx = word_idx
    labels.append(label_ids)
  tokenized_inputs["labels"] = labels
  return tokenized_inputs

"""Verifying our function on a single training example"""

single_sample = panx_hi["train"].select(range(1))
single_sample_encoded = single_sample.map(tokenize_and_align_labels, batched=True)

""" Decoding the training example from the input_ids"""
print(" ".join(token for token in single_sample[0]["tokens"]))
print(xlmr_tokenizer.decode(single_sample_encoded["input_ids"][0]))

original_labels = single_sample["ner_tags_str"][0]
reconstructed_labels = [index2tag[idx] for idx in single_sample_encoded["labels"][0] if idx != -100]
print(original_labels)
print(reconstructed_labels)

"""Encoding each split"""

def encode_panx_dataset(corpus):
  return corpus.map(tokenize_and_align_labels, batched=True, remove_columns=['langs', 'ner_tags', 'tokens'])

panx_hi_encoded = encode_panx_dataset(panx_ch["hi"])
panx_hi_encoded["train"]

"""### Performance Measures
seqeval is a Python framework for sequence labeling evaluation. seqeval can evaluate the performance of chunking tasks such as named-entity recognition, part-of-speech tagging, semantic role labeling etc.

<br>

Precision <br> Recall <br> F1-score
"""

!pip install seqeval

"""Testing out the library -"""

from seqeval.metrics import classification_report
y_true = [["O", "O", "O", "B-MISC", "I-MISC", "I-MISC", "O"], ["B-PER", "I-PER", "O"]]
y_pred = [["O", "O", "B-MISC", "I-MISC", "I-MISC", "I-MISC", "O"], ["B-PER", "I-PER", "O"]]
print(classification_report(y_true, y_pred))

"""So, we make a function that can take the outputs of the model and convert them into lists that seqeval can expect."""

import numpy as np
def align_predictions(predictions, label_ids):
  preds = np.argmax(predictions, axis=2)
  batch_size, seq_len = preds.shape
  labels_list, preds_list = [], []
  for batch_idx in range(batch_size):
    example_labels, example_preds = [], []
    for seq_idx in range(seq_len):
  # ignore label IDs = -100
      if label_ids[batch_idx, seq_idx] != -100:
        example_labels.append(index2tag[label_ids[batch_idx][seq_idx]])
        example_preds.append(index2tag[preds[batch_idx][seq_idx]])
    labels_list.append(example_labels)
    preds_list.append(example_preds)
  return preds_list, labels_list

"""## Fine Tuning XLM-RoBERTa

Our first strategy will be to fine-tune our base model on
the German subset of PAN-X and then evaluate it’s zero-shot cross-lingual performance on French, Italian, and
English

F1 Score = (2 * Precision * Recall) / (Precision + Recall)
"""

!sudo apt-get install git-lfs

#!huggingface-cli login

#!git config --global credential.helper store

from transformers import TrainingArguments
num_epochs = 3
batch_size = 24
logging_steps = len(panx_hi_encoded["train"]) // batch_size
training_args = TrainingArguments(
    #hub_token="USE YOUR TOKEN",
    #push_to_hub=True,
    output_dir="xlm-roberta-base-finetuned-panx-hi",
 num_train_epochs=num_epochs,
 per_device_train_batch_size=batch_size,
 per_device_eval_batch_size=batch_size,
 evaluation_strategy="epoch", save_steps=1e6,
 weight_decay=0.01, disable_tqdm=False,
 logging_steps=logging_steps)

from seqeval.metrics import f1_score
def compute_metrics(eval_pred):
 y_pred, y_true = align_predictions(eval_pred.predictions,
 eval_pred.label_ids)
 return {"f1": f1_score(y_true, y_pred)}

"""Training took approximately 1 hours"""

from transformers import DataCollatorForTokenClassification
data_collator = DataCollatorForTokenClassification(xlmr_tokenizer)

from transformers import Trainer
trainer = Trainer(model=xlmr_model, args=training_args,
 data_collator=data_collator, compute_metrics=compute_metrics,
 train_dataset=panx_hi_encoded["train"],
 eval_dataset=panx_hi_encoded["validation"],
 tokenizer=xlmr_tokenizer)

trainer.train()

"""Saving the model in huggingface"""

!git lfs install

!git config --global user.email "mehershrishtinigam@gmail.com"

!git config --global user.name "shrishtinigam"

trainer.save_model("")

!ls

!cd xlm-roberta-base-finetuned-panx-hi

!git add .
!git commit -m "Initial commit"
!git push

"""To test our model works, lets test it on the Hindi translation of the simple example -"""

text_de = "जेफ डीन कैलिफोर्निया में एप्पल में कंप्यूटर साइंस इंजीनियर हैं"
tag_text(text_de, tags, trainer.model, xlmr_tokenizer)

"""## Error Analysis"""

!pip install datasets transformers[sentencepiece]

import torch.nn as nn
from transformers import XLMRobertaConfig
from transformers.models.roberta.modeling_roberta import (RobertaModel, RobertaPreTrainedModel)
class XLMRobertaForTokenClassification(RobertaPreTrainedModel):
  config_class = XLMRobertaConfig
  def __init__(self, config):
    super().__init__(config)
    self.num_labels = config.num_labels
    # load model body
    self.roberta = RobertaModel(config, add_pooling_layer=False)
    # setup token classification head
    self.dropout = nn.Dropout(config.hidden_dropout_prob)
    self.classifier = nn.Linear(config.hidden_size, config.num_labels)
    # load and initialize weights
    self.init_weights()
  def forward(self, input_ids=None, attention_mask=None,
 token_type_ids=None, labels=None, **kwargs):
    pass

#xlmr_model = (XLMRobertaForTokenClassification
 #.from_pretrained("models/xlm-roberta-base-finetuned-panx-hi")
 #.to(device))

"""Iterating over validation set"""

from torch.nn.functional import cross_entropy
def forward_pass_with_label(batch):
  # convert dict of lists to list of dicts
  features = [dict(zip(batch, t)) for t in zip(*batch.values())]
  # pad inputs and labels
  batch = data_collator(features)
  input_ids = batch["input_ids"].to(device)
  attention_mask = batch["attention_mask"].to(device)
  labels = batch["labels"].to(device)
  with torch.no_grad():
    output = xlmr_model(input_ids, attention_mask)
    batch["predicted_label"] = torch.argmax(output.logits, axis=-1)
  loss = cross_entropy(output.logits.view(-1, 7),
  labels.view(-1), reduction="none")
  loss = loss.view(len(input_ids), -1)
  batch["loss"] = loss
  # datasets requires list of NumPy array data types
  for k, v in batch.items():
    batch[k] = v.cpu().numpy()
  return batch

valid_set = panx_hi_encoded["validation"]
valid_set = valid_set.map(forward_pass_with_label, batched=True, batch_size=32)
valid_set.set_format("pandas")
df = valid_set[:]

"""Labels are encoded with their ID's so we convert them to strings to make it easier to read results."""

index2tag[-100] = "IGN"
df["input_tokens"] = df["input_ids"].apply(
 lambda x: xlmr_tokenizer.convert_ids_to_tokens(x))
df["predicted_label"] = df["predicted_label"].apply(
 lambda x: [index2tag[i] for i in x])
df["labels"] = df["labels"].apply(lambda x: [index2tag[i] for i in x])

df_tokens = df.apply(pd.Series.explode)
df_tokens = df_tokens.query("labels != 'IGN'")
df_tokens["loss"] = df_tokens["loss"].astype(float)

(
 df_tokens.groupby("input_tokens")[["loss"]]
 .agg(["count", "mean", "sum"])
 .droplevel(level=0, axis=1) # get rid of multi-level columns
 .sort_values(by="sum", ascending=False)
 .reset_index()
 .head(20)
)

df_tokens.query("input_tokens == '_West'")["labels"].value_counts()

(
 df_tokens.groupby("labels")[["loss"]]
 .agg(["count", "mean", "sum"])
 .droplevel(level=0, axis=1)
 .sort_values(by="mean", ascending=False)
 .reset_index()
)

from sklearn.metrics import ConfusionMatrixDisplay
ConfusionMatrixDisplay.from_predictions(df_tokens["labels"], df_tokens["predicted_label"])

def display_samples(df):
  for _, row in df.iterrows():
    labels, preds, tokens, losses = [], [], [], []
    for i, mask in enumerate(row["attention_mask"]):
      if mask == 1:
        labels.append(row["labels"][i])
        preds.append(row["predicted_label"][i])
        tokens.append(row["input_tokens"][i])
        losses.append(f"{row['loss'][i]:.2f}")
      df_tmp = pd.DataFrame({"tokens": tokens, "labels": labels,"preds": preds, "losses": losses}).T
      df_tmp

df["total_loss"] = df["loss"].apply(sum)
display_samples(df.sort_values(by="total_loss", ascending=False).head(3))

display_samples(df.loc[df["input_tokens"].apply(lambda x: "_(" in x)].head(3))

panx_hi_encoded["validation"].reset_format()
preds_valid = trainer.predict(panx_hi_encoded["validation"])

preds_valid.metrics

def generate_report(trainer, dataset):
 preds = trainer.predict(dataset)
 preds_list, label_list = align_predictions(preds.predictions, preds.label_ids)
 print(classification_report(label_list, preds_list, digits=4))
 print(preds.metrics)
 return 1 #preds.metrics["eval_f1"]

from collections import defaultdict
f1_scores = defaultdict(dict)
f1_scores["hi"]["hi"] = generate_report(trainer, panx_hi_encoded["test"])

text_fr = "जेफ डीन कैलिफोर्निया में एप्पल में कंप्यूटर साइंस इंजीनियर हैं"
tag_text(text_fr, tags, trainer.model, xlmr_tokenizer)

def evaluate_zero_shot_performance(lang, trainer):
 panx_ds = encode_panx_dataset(panx_ch[lang])
 return generate_report(trainer, panx_ds["test"])
f1_scores["hi"]["ta"] = evaluate_zero_shot_performance("ta", trainer)

f1_scores["hi"]["mr"] = evaluate_zero_shot_performance("mr", trainer)

f1_scores["hi"]["bn"] = evaluate_zero_shot_performance("bn", trainer)